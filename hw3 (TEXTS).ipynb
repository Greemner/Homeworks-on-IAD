{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "name": "HW3 MINE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRP_6GQCbhSb"
      },
      "source": [
        "# –î–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ 3. \n",
        "\n",
        "## –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏ –æ—Ç–µ–ª—è –ø–æ —Ç–µ–∫—Å—Ç—É –æ—Ç–∑—ã–≤–∞."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBGQnVPvbhSd"
      },
      "source": [
        "–ú—ã —Å–æ–±—Ä–∞–ª–∏ –¥–ª—è –≤–∞—Å –æ—Ç–∑—ã–≤—ã –ø–æ 1500 –æ—Ç–µ–ª—è–º –∏–∑ —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ —Ä–∞–∑–Ω—ã—Ö —É–≥–æ–ª–∫–æ–≤ –º–∏—Ä–∞. –ß—Ç–æ —ç—Ç–æ –∑–∞ –æ—Ç–µ–ª–∏ - —Å–µ–∫—Ä–µ—Ç. –í–∞–º –¥–∞–Ω —Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞ –∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –æ—Ç–µ–ª—è. –í–∞—à–∞ –∑–∞–¥–∞—á–∞ - –Ω–∞—É—á–∏—Ç—å—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –æ—Ü–µ–Ω–∫—É –æ—Ç–µ–ª—è –ø–æ –æ—Ç–∑—ã–≤—É. –î–∞–Ω–Ω—ã–µ –º–æ–∂–Ω–æ —Å–∫–∞—á–∞—Ç—å [—Ç—É—Ç](https://www.kaggle.com/c/hseds-texts-2020/data?select=train.csv)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MW1pngS1bhSe"
      },
      "source": [
        "–ì–ª–∞–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ - Mean Absolute Error (MAE). –í–æ –≤—Å–µ—Ö —á–∞—Å—Ç—è—Ö –¥–æ–º–∞—à–Ω–µ–π —Ä–∞–±–æ—Ç—ã –≤–∞–º –Ω—É–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ MAE –Ω–µ –ø—Ä–µ–≤—ã—à–∞—é—â–µ–µ 1. –í –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ –º—ã –±—É–¥–µ–º –≤—ã–Ω—É–∂–¥–µ–Ω—ã –Ω–µ –∑–∞—Å—á–∏—Ç–∞—Ç—å –∑–∞–¥–∞–Ω–∏–µ :( "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZ-JjRe7bhSe"
      },
      "source": [
        "–î–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –≤–∞—à–µ–π –º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ train –∏ test –∏ –∑–∞–º–µ—Ä—è–π—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π —á–∞—Å—Ç–∏."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPgGEFaEbhSf"
      },
      "source": [
        "#### –ü—Ä–æ –¥–∞–Ω–Ω—ã–µ:\n",
        "–ö–∞–∂–¥–æ–µ —Ä–µ–≤—å—é —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –¥–≤—É—Ö —Ç–µ–∫—Å—Ç–æ–≤: positive –∏ negative - –ø–ª—é—Å—ã –∏ –º–∏–Ω—É—Å—ã –æ—Ç–µ–ª—è. –í —Å—Ç–æ–ª–±—Ü–µ score –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –æ—Ü–µ–Ω–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è - –≤–µ—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ 0 –¥–æ 10. –í–∞–º –Ω—É–∂–Ω–æ –∏–∑–≤–ª–µ—á—å –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ —ç—Ç–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –ø–æ –Ω–∏–º –æ—Ü–µ–Ω–∫—É."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBmY8athbhSf"
      },
      "source": [
        "–£–¥–∞—á–∏! üí™"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TUBNl3ibhSf"
      },
      "source": [
        "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–Ω–µ—à–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å—Ç—Ä–æ–≥–æ –∑–∞–ø—Ä–µ—â–µ–Ω–æ. –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–∑ torchvision."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaknMi1jfsl9",
        "outputId": "bc3ab558-a4f6-4868-f7e6-10ff708854de"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raYF-39obhSf"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "PATH_TO_TRAIN_DATA = '/content/drive/MyDrive/train.csv'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJ1AwxO3bhSh"
      },
      "source": [
        "import string\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "def process_text(text):\n",
        "    if len(text.split(' ')) == 0:\n",
        "        return 'empty empty empty'\n",
        "    words = []\n",
        "    for word in word_tokenize(text.lower()):\n",
        "        if word not in string.punctuation:\n",
        "            #word = ps.stem(word)\n",
        "            words.append(word)\n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "yIf8KZwubhSg",
        "outputId": "99b13b0b-f018-4245-f299-fb1770a84801"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(PATH_TO_TRAIN_DATA)\n",
        "df['opinion'] = df['positive'].str.cat(df['negative'], sep =\" \")\n",
        "df = df.drop('review_id', axis=1)\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100000, 4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>negative</th>\n",
              "      <th>positive</th>\n",
              "      <th>score</th>\n",
              "      <th>opinion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>There were issues with the wifi connection</td>\n",
              "      <td>No Positive</td>\n",
              "      <td>7.1</td>\n",
              "      <td>No Positive  There were issues with the wifi c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TV not working</td>\n",
              "      <td>No Positive</td>\n",
              "      <td>7.5</td>\n",
              "      <td>No Positive  TV not working</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>More pillows</td>\n",
              "      <td>Beautiful room Great location Lovely staff</td>\n",
              "      <td>10.0</td>\n",
              "      <td>Beautiful room Great location Lovely staff   ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Very business</td>\n",
              "      <td>Location</td>\n",
              "      <td>5.4</td>\n",
              "      <td>Location  Very business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Rooms could do with a bit of a refurbishment ...</td>\n",
              "      <td>Nice breakfast handy for Victoria train stati...</td>\n",
              "      <td>6.7</td>\n",
              "      <td>Nice breakfast handy for Victoria train stati...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            negative  ...                                            opinion\n",
              "0         There were issues with the wifi connection  ...  No Positive  There were issues with the wifi c...\n",
              "1                                     TV not working  ...                        No Positive  TV not working\n",
              "2                                       More pillows  ...   Beautiful room Great location Lovely staff   ...\n",
              "3                                      Very business  ...                            Location  Very business\n",
              "4   Rooms could do with a bit of a refurbishment ...  ...   Nice breakfast handy for Victoria train stati...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfB2zD6YbhSg"
      },
      "source": [
        "–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –º–æ–∂–µ—Ç —Å–∫–∞–∑—ã–≤–∞—Ç—å—Å—è –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–µ –≤–∞—à–µ–π –º–æ–¥–µ–ª–∏.\n",
        "–°–¥–µ–ª–∞–µ–º –Ω–µ–±–æ–ª—å—à–æ–π –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ —Ç–µ–∫—Å—Ç–æ–≤: —É–¥–∞–ª–∏–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è, –ø—Ä–∏–≤–µ–¥–µ–º –≤—Å–µ —Å–ª–æ–≤–∞ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É. \n",
        "–û–¥–Ω–∞–∫–æ –º–æ–∂–Ω–æ –Ω–µ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—Ç—å—Å—è —ç—Ç–∏–º –Ω–∞–±–æ—Ä–æ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π. –ü–æ–¥—É–º–∞–π—Ç–µ, —á—Ç–æ –µ—â–µ –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å —Å —Ç–µ–∫—Å—Ç–∞–º–∏, —á—Ç–æ–±—ã –ø–æ–º–æ—á—å –±—É–¥—É—â–∏–º –º–æ–¥–µ–ª—è–º? –î–æ–±–∞–≤—å—Ç–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥–ª–∏ –±—ã –ø–æ–º–æ—á—å –ø–æ –≤–∞—à–µ–º—É –º–Ω–µ–Ω–∏—é."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCIuKMucbhSh"
      },
      "source": [
        "–¢–∞–∫–∂–µ –º—ã –¥–æ–±–∞–≤–∏–ª–∏ —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ –Ω–∞ —Ç–æ–∫–µ–Ω—ã. –¢–µ–ø–µ—Ä—å –∫–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞-—Ä–µ–≤—å—é —Å—Ç–∞–ª–∞ –º–∞—Å—Å–∏–≤–æ–º —Ç–æ–∫–µ–Ω–æ–≤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7INSD5NnbhSh"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_train, df_test = train_test_split(df, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDHhj23WbhSi"
      },
      "source": [
        "### –ß–∞—Å—Ç—å 1. 1 –±–∞–ª–ª"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl8l_2AcbhSi"
      },
      "source": [
        "–û–±—É—á–∏—Ç–µ –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é –Ω–∞ TF-IDF –≤–µ–∫—Ç–æ—Ä–∞—Ö —Ç–µ–∫—Å—Ç–æ–≤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBp1n-ZXbhSi"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import Ridge, LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from scipy import sparse\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5J5jGFZIjZF"
      },
      "source": [
        "tfidf = TfidfVectorizer(max_features=2000, tokenizer=process_text)\r\n",
        "\r\n",
        "positive_train = tfidf.fit_transform(df_train['positive'])\r\n",
        "positive_test = tfidf.transform(df_test['positive'])\r\n",
        "negative_train = tfidf.fit_transform(df_train['negative'])\r\n",
        "negative_test = tfidf.transform(df_test['negative'])\r\n",
        "\r\n",
        "x_train = sparse.hstack((positive_train, negative_train))\r\n",
        "y_train = df_train['score']\r\n",
        "x_test = sparse.hstack((positive_test, negative_test))\r\n",
        "y_test = df_test['score']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Pg6UTjHpRpmA",
        "outputId": "d8f9b741-7f97-4b8f-fa63-187f84d2b15c"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>negative</th>\n",
              "      <th>positive</th>\n",
              "      <th>score</th>\n",
              "      <th>opinion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>98980</th>\n",
              "      <td>Cups washed up each day they stayed dirty all...</td>\n",
              "      <td>No Positive</td>\n",
              "      <td>4.2</td>\n",
              "      <td>[no, positive, cups, washed, up, each, day, th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69824</th>\n",
              "      <td>Tiny but tidy cosy pool</td>\n",
              "      <td>Good location friendly staff</td>\n",
              "      <td>8.8</td>\n",
              "      <td>[good, location, friendly, staff, tiny, but, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9928</th>\n",
              "      <td>No Negative</td>\n",
              "      <td>everything good location really nice room</td>\n",
              "      <td>8.3</td>\n",
              "      <td>[everything, good, location, really, nice, roo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75599</th>\n",
              "      <td>The walls between the rooms are a bit thin Yo...</td>\n",
              "      <td>The linen was always fresh Never felt like an...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>[the, linen, was, always, fresh, never, felt, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95621</th>\n",
              "      <td>Nothing</td>\n",
              "      <td>Excellent location beside the underground</td>\n",
              "      <td>9.6</td>\n",
              "      <td>[excellent, location, beside, the, underground...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                negative  ...                                            opinion\n",
              "98980   Cups washed up each day they stayed dirty all...  ...  [no, positive, cups, washed, up, each, day, th...\n",
              "69824                            Tiny but tidy cosy pool  ...  [good, location, friendly, staff, tiny, but, t...\n",
              "9928                                         No Negative  ...  [everything, good, location, really, nice, roo...\n",
              "75599   The walls between the rooms are a bit thin Yo...  ...  [the, linen, was, always, fresh, never, felt, ...\n",
              "95621                                            Nothing  ...  [excellent, location, beside, the, underground...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIopp9DdvzaK",
        "outputId": "899ae75d-0626-4331-da16-14406e386a75"
      },
      "source": [
        "model = Ridge(alpha=5, max_iter=50)\r\n",
        "model.fit(x_train, y_train)\r\n",
        "y_pred = model.predict(x_test)\r\n",
        "error = mean_absolute_error(y_test, y_pred) \r\n",
        "print('MAE Ridge:', round(error, 6))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAE Ridge: 0.821561\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TSJbfDN06QI",
        "outputId": "4b63dfbf-8a8d-43b5-8888-ae804c43728d"
      },
      "source": [
        "model = LogisticRegression(max_iter=1000)\r\n",
        "model.fit(x_train, y_train.mul(10).astype(int))\r\n",
        "y_pred = model.predict_proba(x_test)\r\n",
        "y_pred = (y_pred * model.classes_).sum(axis=1) / 10\r\n",
        "error = mean_absolute_error(y_test, y_pred) \r\n",
        "print('MAE LogReg:', round(error, 6))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO - 08:02:34: NumExpr defaulting to 2 threads.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MAE LogReg: 0.80854\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM5lPD73bhSi"
      },
      "source": [
        "### –ß–∞—Å—Ç—å 2. 3 –±–∞–ª–ª–∞"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap1QSXqKbhSi"
      },
      "source": [
        "–û–±—É—á–∏—Ç–µ –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é –Ω–∞ —É—Å—Ä–µ–¥–Ω–µ–Ω–Ω—ã—Ö Word2Vec –≤–µ–∫—Ç–æ—Ä–∞—Ö. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zq03HmQZLStb"
      },
      "source": [
        "from gensim.models import Word2Vec\r\n",
        "from gensim.models.phrases import Phrases, Phraser\r\n",
        "import logging\r\n",
        "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7dy2hd8LSv8"
      },
      "source": [
        "EMB_SIZE = 500\r\n",
        "\r\n",
        "w2v_model = Word2Vec(min_count=1,\r\n",
        "                     window=2,\r\n",
        "                     size=EMB_SIZE,\r\n",
        "                     sample=6e-5, \r\n",
        "                     alpha=0.03, \r\n",
        "                     min_alpha=0.0007, \r\n",
        "                     negative=20,\r\n",
        "                     workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmLXr37HLSye"
      },
      "source": [
        "w2v_model.build_vocab(df['opinion'], progress_per=50000)\r\n",
        "w2v_model.train(df['opinion'], total_examples=w2v_model.corpus_count, epochs=20, report_delay=1)\r\n",
        "w2v_model.init_sims(replace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXrarR8hLS6Q",
        "outputId": "9c8d78a5-3346-42b0-a2c5-5f6280e30204"
      },
      "source": [
        "w2v_model.wv.most_similar(positive=[\"good\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('great', 0.7073086500167847),\n",
              " ('excellent', 0.6422793865203857),\n",
              " ('nice', 0.5858272910118103),\n",
              " ('decent', 0.5412341952323914),\n",
              " ('poor', 0.508098840713501),\n",
              " ('very', 0.4963108003139496),\n",
              " ('comfortable', 0.4706449508666992),\n",
              " ('superb', 0.46917957067489624),\n",
              " ('plentiful', 0.4450289309024811),\n",
              " ('tasty', 0.44377583265304565)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32xe0sB0LS8f"
      },
      "source": [
        "def embed_mean(model, data, emb):\r\n",
        "    df_vectors = np.zeros((1, emb))\r\n",
        "    sentences_vectors = np.zeros((1, emb))\r\n",
        "    for sentence in data:\r\n",
        "        sent_vectors = np.zeros_like((2, emb))\r\n",
        "        w = 0\r\n",
        "        for word in sentence:\r\n",
        "            \r\n",
        "            try:\r\n",
        "                if w == 0:\r\n",
        "                    word_vector = np.array(model[word])\r\n",
        "                    sent_vectors = word_vector\r\n",
        "                else:\r\n",
        "                    word_vector = np.array(model[word])\r\n",
        "                    sent_vectors = np.vstack((sent_vectors, word_vector))\r\n",
        "                w += 1\r\n",
        "            except:\r\n",
        "                w += 1\r\n",
        "                continue\r\n",
        "        try:\r\n",
        "            if sentences_vectors.shape[0] == 1:\r\n",
        "                sentences_vectors = np.mean(sent_vectors, axis=0)\r\n",
        "            else:\r\n",
        "                sentences_vectors = np.vstack((sentences_vectors, np.mean(sent_vectors, axis=0)))\r\n",
        "        except:\r\n",
        "            sentences_vectors = np.vstack((sentences_vectors, np.zeros((1, emb))))\r\n",
        "        \r\n",
        "        if sentences_vectors.shape[0] > 2500:\r\n",
        "            if df_vectors.shape[0] == 1:\r\n",
        "                df_vectors = sentences_vectors\r\n",
        "            else:\r\n",
        "                df_vectors = np.vstack((df_vectors, sentences_vectors))\r\n",
        "            sentences_vectors = np.zeros((1, emb))\r\n",
        "\r\n",
        "    if df_vectors.shape[0] < data.shape[0]:\r\n",
        "        df_vectors = np.vstack((df_vectors, sentences_vectors))\r\n",
        "\r\n",
        "    return df_vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09IFzdk-LS_h",
        "outputId": "8ffa89b1-2d2b-4008-ca22-2986562152b5"
      },
      "source": [
        "mean_df = embed_mean(w2v_model, df['opinion'], EMB_SIZE)\r\n",
        "mean_df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100000, 500)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2fg3SiPRb8X",
        "outputId": "0b518d2b-0fca-497c-b801-3782734b2c65"
      },
      "source": [
        "y = df['score']\r\n",
        "x_train, x_test, y_train, y_test = train_test_split(mean_df, y, random_state=42)\r\n",
        "\r\n",
        "model = LogisticRegression(penalty='l2', max_iter=2000, C=1)\r\n",
        "model.fit(x_train, y_train.mul(10).astype(int))\r\n",
        "y_pred = model.predict_proba(x_test)\r\n",
        "y_pred = (y_pred * model.classes_).sum(axis=1) / 10\r\n",
        "error = mean_absolute_error(y_test, y_pred) \r\n",
        "print('MAE LogReg on embeddings:', round(error, 6))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAE LogReg on embeddings: 0.962973\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcI0w4j-bhSj"
      },
      "source": [
        "–£—Å—Ä–µ–¥–Ω—è—è w2v –≤–µ–∫—Ç–æ—Ä–∞, –º—ã –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –∏–º–µ–µ—Ç —Ä–∞–≤–Ω–æ—Ü–µ–Ω–Ω—ã–π –≤–∫–ª–∞–¥ –≤ —Å–º—ã—Å–ª –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –æ–¥–Ω–∞–∫–æ —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ —Å–æ–≤—Å–µ–º —Ç–∞–∫. –¢–µ–ø–µ—Ä—å –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥—Ä—É–≥–æ–π –∫–æ–Ω—Ü–µ–ø—Ü–∏–µ–π –∏ –ø–µ—Ä–µ–≤–∑–≤–µ—Å–∏—Ç—å —Å–ª–æ–≤–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∏—Ç–æ–≥–æ–≤–æ–≥–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —Ç–µ–∫—Å—Ç–∞. –í –∫–∞—á–µ—Å—Ç–≤–µ –≤–µ—Å–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ IDF (Inverse document frequency)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S809APnt2VXH"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "\r\n",
        "df = pd.read_csv(PATH_TO_TRAIN_DATA)\r\n",
        "df['opinion'] = df['positive'].str.cat(df['negative'], sep =\" \")\r\n",
        "\r\n",
        "tfidf = TfidfVectorizer(tokenizer=process_text)\r\n",
        "\r\n",
        "_ = tfidf.fit_transform(df['opinion'])\r\n",
        "\r\n",
        "df['opinion'] = df['opinion'].apply(process_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWBNfLou1yyR"
      },
      "source": [
        "def embed_idf(model, data, tfidf_w, tfidf_i, emb):\r\n",
        "    df_vectors = np.zeros((1, emb))\r\n",
        "    sentences_vectors = np.zeros((1, emb))\r\n",
        "    for sentence in data:\r\n",
        "        sent_vectors = np.zeros_like((2, emb))\r\n",
        "\r\n",
        "        idf_arr = np.array([])\r\n",
        "        w = 0\r\n",
        "        for word in sentence:\r\n",
        "            \r\n",
        "            try:\r\n",
        "                index = tfidf_w.index(word)\r\n",
        "                idf_arr = np.append(idf_arr, tfidf_i[index])\r\n",
        "                if w == 0:\r\n",
        "                    word_vector = np.array(model[word])\r\n",
        "                    sent_vectors = word_vector\r\n",
        "                else:\r\n",
        "                    word_vector = np.array(model[word])\r\n",
        "                    sent_vectors = np.vstack((sent_vectors, word_vector))\r\n",
        "                w += 1\r\n",
        "            except:\r\n",
        "                w += 1\r\n",
        "                continue\r\n",
        "        try:\r\n",
        "            idf_arr = (idf_arr / np.sum(idf_arr)).T\r\n",
        "            idf_arr = np.reshape(idf_arr, (idf_arr.shape[0], 1))\r\n",
        "            if sentences_vectors.shape[0] == 1:\r\n",
        "                sentences_vectors = np.sum(sent_vectors * idf_arr, axis=0)\r\n",
        "            else:\r\n",
        "                sentences_vectors = np.vstack((sentences_vectors, np.sum(sent_vectors * idf_arr, axis=0)))\r\n",
        "        except:\r\n",
        "            sentences_vectors = np.vstack((sentences_vectors, np.zeros((1, emb))))\r\n",
        "        \r\n",
        "        if sentences_vectors.shape[0] > 2500:\r\n",
        "            if df_vectors.shape[0] == 1:\r\n",
        "                df_vectors = sentences_vectors\r\n",
        "            else:\r\n",
        "                df_vectors = np.vstack((df_vectors, sentences_vectors))\r\n",
        "            sentences_vectors = np.zeros((1, emb))\r\n",
        "\r\n",
        "    if df_vectors.shape[0] < 100000:\r\n",
        "        df_vectors = np.vstack((df_vectors, sentences_vectors))\r\n",
        "\r\n",
        "    return df_vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brfuVaev81mv",
        "outputId": "27b296ac-ba35-4816-cf9e-44c15d5a0cc6"
      },
      "source": [
        "tfidf_i = tfidf.idf_\r\n",
        "tfidf_w = list(tfidf.vocabulary_.keys())\r\n",
        "\r\n",
        "idf_df = embed_idf(w2v_model, df['opinion'], tfidf_w, tfidf_i, EMB_SIZE)\r\n",
        "idf_df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100000, 500)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yG75WudpTSJT",
        "outputId": "bbafb7d2-6f12-49d7-d370-c0e3a89ba05e"
      },
      "source": [
        "y = df['score']\r\n",
        "x_train, x_test, y_train, y_test = train_test_split(idf_df, y, random_state=42)\r\n",
        "\r\n",
        "model = LogisticRegression(penalty='l2', max_iter=2000, C=1)\r\n",
        "model.fit(x_train, y_train.mul(10).astype(int))\r\n",
        "y_pred = model.predict_proba(x_test)\r\n",
        "y_pred = (y_pred * model.classes_).sum(axis=1) / 10\r\n",
        "error = mean_absolute_error(y_test, y_pred) \r\n",
        "print('MAE LogReg on weighted embeddings:', round(error, 6))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAE LogReg on weighted embeddings: 0.962061\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpqvUsrAbhSj"
      },
      "source": [
        "–ü—Ä–æ–≤–µ–¥–∏—Ç–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é —ç–º–±–µ–¥–¥–∏–Ω–≥–∞. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑ –¥–≤—É—Ö –º–µ—Ç–æ–¥–æ–≤ –ø–æ—Å—Ç—Ä–æ–π—Ç–µ –≥—Ä–∞—Ñ–∏–∫ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞. \n",
        "#### –°–¥–µ–ª–∞–π—Ç–µ –≤—ã–≤–æ–¥—ã:\n",
        "**–ß–µ–º –±–æ–ª—å—à–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–∏–Ω–≥–∞, —Ç–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ –ª—É—á—à–µ. –ü—Ä–∏ —ç—Ç–æ–º —É—Å—Ä–µ–¥–Ω–µ–Ω–Ω—ã–µ –∏ –≤–∑–≤–µ—à–µ–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä–∞ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–∏–º–µ—Ä–Ω–æ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv26AwDwbhSj"
      },
      "source": [
        "–¢–µ–ø–µ—Ä—å –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –æ–±—É—á–∏—Ç—å –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é –Ω–∞ –ª—é–±—ã—Ö –¥—Ä—É–≥–∏—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ 300 –∏ —Å—Ä–∞–≤–Ω–∏—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ —Å Word2Vec.\n",
        "#### –í—ã–≤–æ–¥—ã:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7i86kFQ7bhSk"
      },
      "source": [
        "### –ß–∞—Å—Ç—å 3. 6 –±–∞–ª–ª–æ–≤"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLPl8lK6bhSk"
      },
      "source": [
        "–¢–µ–ø–µ—Ä—å –¥–∞–≤–∞–π—Ç–µ –≤–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è –±–æ–ª–µ–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –ø—Ä–æ—Ö–æ–¥–∏–ª–∏ –≤ –Ω–∞—à–µ–º –∫—É—Ä—Å–µ. –û–±—É—á–∏—Ç–µ RNN/Transformer –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏. –ü–æ–ª—É—á–∏—Ç–µ –æ—à–∏–±–∫—É –º–µ–Ω—å—à–µ, —á–µ–º –≤–æ –≤—Å–µ—Ö –≤—ã—à–µ–ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–∞—Ö."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngv93ei2bhSk"
      },
      "source": [
        "–ï—Å–ª–∏ –±—É–¥–µ—Ç–µ –æ–±—É—á–∞—Ç—å RNN, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è. –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –æ—Ç–∑—ã–≤—ã –º–æ–≥—É—Ç –±—ã—Ç—å —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö.\n",
        "\n",
        "–ß—Ç–æ–±—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è DataLoader, –≤—Å–µ –µ–≥–æ —ç–ª–µ–º–µ–Ω—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏. –î–ª—è —ç—Ç–æ–≥–æ –≤—ã –º–æ–∂–µ—Ç–µ –¥–æ–±–∞–≤–∏—Ç—å –Ω—É–ª–µ–≤–æ–π –ø–∞–¥–¥–∏–Ω–≥ –∫–æ –≤—Å–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º (—Å–º –ø—Ä–∏–º–µ—Ä pad_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj8pctOVbhSk"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch import autograd\n",
        "import spacy\n",
        "import torch.optim as optim\n",
        "import spacy\n",
        "from torchtext.data import Field, TabularDataset, BucketIterator\n",
        "import pandas as pd"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzf0AMHCeb9l"
      },
      "source": [
        "df = pd.read_csv(PATH_TO_TRAIN_DATA)\r\n",
        "df['opinion'] = df['positive'].str.cat(df['negative'], sep =\" \")\r\n",
        "df = df.drop(['review_id', 'positive', 'negative'], axis=1)\r\n",
        "\r\n",
        "df_train, df_test = train_test_split(df, random_state=42)\r\n",
        "df_train.to_csv('/content/drive/MyDrive/TrainSet.csv', index=False)\r\n",
        "df_test.to_csv('/content/drive/MyDrive/TestSet.csv', index=False)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZ6OKHM9aCwj"
      },
      "source": [
        "spacy_en = spacy.load(\"en\")\r\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "\r\n",
        "def tokenize(text):\r\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\r\n",
        "\r\n",
        "\r\n",
        "opinion = Field(sequential=True, use_vocab=True, tokenize=tokenize, lower=True)\r\n",
        "score = Field(sequential=False, use_vocab=False, is_target=True, dtype=torch.float)\r\n",
        "\r\n",
        "fields = {\"opinion\": (\"text\", opinion), \"score\": (\"score\", score)}\r\n",
        "\r\n",
        "train_data, test_data = TabularDataset.splits(path='/content/drive/MyDrive/', train='TrainSet.csv', test='TestSet.csv', format='csv', fields=fields)\r\n",
        "\r\n",
        "opinion.build_vocab(train_data, max_size=20000, min_freq=5)\r\n",
        "\r\n",
        "train_iterator, test_iterator = BucketIterator.splits(\r\n",
        "    (train_data, test_data), batch_size=256, device=device,\r\n",
        "    sort_within_batch = True, sort_key = lambda x: len(x.text))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS2_snP8Z7KS"
      },
      "source": [
        "class RNN_LSTM(nn.Module):\r\n",
        "    def __init__(self, input_size, embed_size, hidden_size, num_layers):\r\n",
        "        super(RNN_LSTM, self).__init__()\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "        self.num_layers = num_layers\r\n",
        "\r\n",
        "        self.embedding = nn.Embedding(input_size, embed_size)\r\n",
        "        self.rnn = nn.LSTM(embed_size, hidden_size, num_layers)\r\n",
        "        self.fc_out = nn.Linear(hidden_size, 1)\r\n",
        "        \r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        h0 = torch.zeros(self.num_layers, x.size(1), self.hidden_size).to(device)\r\n",
        "        c0 = torch.zeros(self.num_layers, x.size(1), self.hidden_size).to(device)\r\n",
        "\r\n",
        "        embedded = self.embedding(x)\r\n",
        "        outputs, _ = self.rnn(embedded, (h0, c0))\r\n",
        "        prediction = self.fc_out(outputs[-1, :, :])\r\n",
        "\r\n",
        "        return prediction"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFwg_ivzZ7P8"
      },
      "source": [
        "from tqdm import tqdm\r\n",
        "from sklearn.metrics import mean_absolute_error\r\n",
        "import sys\r\n",
        "\r\n",
        "def train_one_epoch(model, train_dataloader, criterion, optimizer, device=\"cuda:0\"):\r\n",
        "    model = model.to(device).train()\r\n",
        "    model.train()\r\n",
        "    total_loss = 0\r\n",
        "    num_batches = 0\r\n",
        "    all_losses = []\r\n",
        "    all_mae = np.array([])\r\n",
        "    for batch_idx, batch in enumerate(train_dataloader):\r\n",
        "        data = batch.text.to(device=device)\r\n",
        "        targets = batch.score.to(device=device)\r\n",
        "\r\n",
        "        scores = model(data)\r\n",
        "        loss = criterion(scores.squeeze(1), targets.type_as(scores))\r\n",
        "\r\n",
        "        optimizer.zero_grad()\r\n",
        "        loss.backward()\r\n",
        "\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        error = mean_absolute_error(targets.cpu().detach().numpy(), scores.cpu().detach().numpy()).mean()\r\n",
        "\r\n",
        "        total_loss += loss.item()\r\n",
        "        all_mae = np.append(all_mae, error)\r\n",
        "        num_batches += 1\r\n",
        "        all_losses.append(loss.detach().item())\r\n",
        "\r\n",
        "    loss = total_loss / num_batches\r\n",
        "    return loss, np.mean(all_mae)\r\n",
        "\r\n",
        "\r\n",
        "def predict(model, test_dataloader, criterion, device=\"cuda:0\"):\r\n",
        "    model = model.to(device).eval()\r\n",
        "    total_loss = 0\r\n",
        "    num_batches = 0\r\n",
        "    all_mae = np.array([])\r\n",
        "    for batch_idx, batch in enumerate(test_dataloader):\r\n",
        "        data = batch.text.to(device=device)\r\n",
        "        targets = batch.score.to(device=device)\r\n",
        "\r\n",
        "        scores = model(data)\r\n",
        "        loss = criterion(scores.squeeze(1), targets.type_as(scores))\r\n",
        "        error = mean_absolute_error(targets.cpu().detach().numpy(), scores.cpu().detach().numpy()).mean()\r\n",
        "\r\n",
        "        total_loss += loss.item()\r\n",
        "        all_mae = np.append(all_mae, error)\r\n",
        "        num_batches += 1\r\n",
        "    losses =  total_loss / num_batches\r\n",
        "    return losses, np.mean(all_mae)\r\n",
        "\r\n",
        "\r\n",
        "def train(model, train_dataloader, val_dataloader, criterion, optimizer, device=\"cuda:0\", n_epochs=10, scheduler=None):\r\n",
        "    model = model.to(device)\r\n",
        "    loss_on_train = np.array([])\r\n",
        "    loss_on_val = np.array([])\r\n",
        "    mae_on_train = np.array([])\r\n",
        "    mae_on_val = np.array([])\r\n",
        "    for epoch in range(n_epochs):\r\n",
        "        print('\\n#############################EPOCH ', epoch + 1, '#############################', sep='')\r\n",
        "        loss1, mae1 = train_one_epoch(model, train_dataloader, criterion, optimizer, device)\r\n",
        "        mae_on_train = np.append(mae_on_train, mae1)\r\n",
        "        loss_on_train = np.append(loss_on_train, loss1)\r\n",
        "\r\n",
        "        with torch.no_grad():\r\n",
        "            loss2, mae2 = predict(model, val_dataloader, criterion, device)\r\n",
        "            mae_on_val = np.append(mae_on_val, mae2)\r\n",
        "            loss_on_val = np.append(loss_on_val, loss2)\r\n",
        "            \r\n",
        "        print('Train loss: ', round(loss1, 6), 'Val loss: ', round(loss2, 6), 'Train MAE: ', round(mae1, 4), 'Val MAE: ', round(mae2, 4))\r\n",
        "\r\n",
        "        if scheduler is not None:\r\n",
        "            scheduler.step()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7x5fRZBEZ7ND"
      },
      "source": [
        "input_size = len(opinion.vocab)\r\n",
        "hidden_size = 512\r\n",
        "num_layers = 2\r\n",
        "embedding_size = 300\r\n",
        "num_epochs = 10\r\n",
        "\r\n",
        "model = RNN_LSTM(input_size, embedding_size, hidden_size, num_layers).to(device)\r\n",
        "\r\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.05, weight_decay=1e-4)\r\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.7)\r\n",
        "criterion = nn.MSELoss()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYx7IL8gZ7TI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2c2255e-c96c-4432-a2cb-799067fc5940"
      },
      "source": [
        "train(model, train_iterator, test_iterator, criterion, optimizer, scheduler=scheduler)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "#############################EPOCH 1#############################\n",
            "Train loss:  3.744079 Val loss:  3.235187 Train MAE:  1.4486 Val MAE:  1.5578\n",
            "\n",
            "#############################EPOCH 2#############################\n",
            "Train loss:  2.781396 Val loss:  2.857127 Train MAE:  1.3187 Val MAE:  1.2287\n",
            "\n",
            "#############################EPOCH 3#############################\n",
            "Train loss:  2.639892 Val loss:  2.331853 Train MAE:  1.2806 Val MAE:  1.165\n",
            "\n",
            "#############################EPOCH 4#############################\n",
            "Train loss:  2.40022 Val loss:  2.370409 Train MAE:  1.2125 Val MAE:  1.2379\n",
            "\n",
            "#############################EPOCH 5#############################\n",
            "Train loss:  2.21228 Val loss:  2.134879 Train MAE:  1.1495 Val MAE:  1.0977\n",
            "\n",
            "#############################EPOCH 6#############################\n",
            "Train loss:  1.752652 Val loss:  1.539142 Train MAE:  1.0053 Val MAE:  0.934\n",
            "\n",
            "#############################EPOCH 7#############################\n",
            "Train loss:  1.435641 Val loss:  1.762264 Train MAE:  0.9015 Val MAE:  0.9415\n",
            "\n",
            "#############################EPOCH 8#############################\n",
            "Train loss:  1.335749 Val loss:  1.283365 Train MAE:  0.8672 Val MAE:  0.8454\n",
            "\n",
            "#############################EPOCH 9#############################\n",
            "Train loss:  1.200402 Val loss:  1.265612 Train MAE:  0.8196 Val MAE:  0.8131\n",
            "\n",
            "#############################EPOCH 10#############################\n",
            "Train loss:  1.130956 Val loss:  1.183212 Train MAE:  0.7955 Val MAE:  0.8003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFC7oN-JbhSm"
      },
      "source": [
        "### –ë–æ–Ω—É—Å. 10 –±–∞–ª–ª–æ–≤\n",
        "\n",
        "–ü–æ–±–µ–π—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ 0.75 –≤ [—Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–∏](https://www.kaggle.com/c/hseds-texts-2020/leaderboard). –ú–æ–∂–µ—Ç–µ –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –≤—ã—à–µ–ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –∏–ª–∏ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —á—Ç–æ-–Ω–∏–±—É–¥—å –µ—â–µ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4clyOCzTrPT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p71Zn9BEV-BA"
      },
      "source": [
        "### –ü–æ–ª–µ–∑–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã\r\n",
        "1) https://github.com/aladdinpersson/Machine-Learning-Collection\r\n",
        "\r\n",
        "2) https://fasttext.cc/docs/en/supervised-tutorial.html\r\n",
        "\r\n",
        "3) https://stackoverflow.com/questions/61213493/pytorch-lstm-for-multiclass-classification-typeerror-not-supported-between\r\n",
        "\r\n",
        "4) https://www.analyticsvidhya.com/blog/2020/01/first-text-classification-in-pytorch/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo-agGw0WUPH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}